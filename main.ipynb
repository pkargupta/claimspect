{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 2\n",
    "gamma = 3\n",
    "breadth_weight = 0.5\n",
    "pos_rank = 1\n",
    "breadth = 0.7 # mean\n",
    "depth = 0.8 # max\n",
    "\n",
    "(beta * pos_rank)/(gamma * (breadth_weight*breadth + (1-breadth_weight)*depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876127cc514b4a6bba103e14350f2eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"vaccine\"\n",
    "        self.topic = \"Which COVID-19 vaccine is better, Pfizer or Moderna?\"\n",
    "        self.override = False\n",
    "        self.iters = 4\n",
    "        self.split = False\n",
    "        self.model = \"biobert\"\n",
    "        self.override = True\n",
    "        self.dim = 768\n",
    "        self.length = 512\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Aspects & Key Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 5 aspects and their corresponding keywords for the topic \"Which COVID-19 vaccine is better, Pfizer or Moderna?\":\n",
      "\n",
      "---\n",
      "aspect_0: Efficacy\n",
      "aspect_0_keywords: efficacy rate, vaccine effectiveness, protection rate, immune response, clinical trials, vaccine potency, disease prevention, infection rate, transmission reduction, public health\n",
      "aspect_1: Safety\n",
      "aspect_1_keywords: adverse reactions, side effects, allergic reactions, anaphylaxis, serious adverse events, vaccine safety, immunogenicity, tolerability, reactogenicity, pharmacovigilance\n",
      "aspect_2: Cost\n",
      "aspect_2_keywords: cost-effectiveness, pricing strategy, out-of-pocket cost, insurance coverage, healthcare system, economic burden, budget impact, affordability, value-based pricing, reimbursement\n",
      "aspect_3: Convenience\n",
      "aspect_0_keywords: administration route, dosing schedule, storage requirements, transportation, handling, distribution, logistics, vaccination site, patient compliance, user experience\n",
      "aspect_4: Acceptance\n",
      "aspect_4_keywords: public perception, vaccine hesitancy, acceptance rate, vaccine confidence, trust, misinformation, media coverage, social media, community engagement, health literacy\n"
     ]
    }
   ],
   "source": [
    "aspects, keywords = aspect_candidate_gen(args.topic, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilize these phrases to augment the \"expert\" phrases for AutoPhrase before we pre-process it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality_orig.txt\", \"r\") as f:\n",
    "    all_phrases = [w.strip() for w in f.readlines()]\n",
    "\n",
    "    for a in aspects:\n",
    "        all_phrases.append(a.replace(\"_\", \" \"))\n",
    "\n",
    "    all_phrases.extend([w.replace(\"_\", \" \") for a in keywords for w in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality.txt\", \"w\") as f:\n",
    "    for w_id, w in enumerate(all_phrases):\n",
    "        if w_id == (len(all_phrases) - 1):\n",
    "            f.write(f\"{w}\")\n",
    "        else:\n",
    "            f.write(f\"{w}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printable = set(string.printable)\n",
    "# with open('datasets/qa_papers.jsonl', 'r', encoding='utf-8', errors='ignore') as json_file:\n",
    "#     json_list = list(json_file)\n",
    "\n",
    "#     results = []\n",
    "#     for json_str in json_list:\n",
    "#         results.append(json.loads(json_str))\n",
    "\n",
    "\n",
    "# with open(\"datasets/qa_papers.txt\", \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "#     for result in tqdm(results):\n",
    "#         result['Content'] = ''.join(filter(lambda x: x in printable, result['Content'])).replace('\\r', '')\n",
    "#         f.write(f\"Title: {result['Title']}; Abstract: {result['Abstract']}; Paper: {result['Content'].split(' References ')[0]}\")\n",
    "#         if result != results[-1]:\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Dataset & Pre-process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Corpus Pre-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.719s\n",
      "user\t0m14.568s\n",
      "sys\t0m0.575s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 463021\n",
      "max word token id = 34766\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "# of documents = 50\n",
      "# of distinct POS tags = 56\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 34781\n",
      "# of frequent phrases = 37477\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 2979\n",
      "\tThe size of the negative pool = 34233\n",
      "# truth patterns = 45667\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m3.014s\n",
      "user\t0m8.746s\n",
      "sys\t0m0.268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.482s\n",
      "user\t0m12.325s\n",
      "sys\t0m0.495s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/NEW/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.700000\n",
      "\tQ(single-word phrases) >= 1.000000\n",
      "=======\n",
      "POS guided model loaded.\n",
      "# of loaded patterns = 5210\n",
      "# of loaded truth patterns = 48646\n",
      "POS transition matrix loaded\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "POS file doesn't have enough POS tags\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 51071\n",
      "   # of total processed sentences = 79188\n",
      "   avg highlights per sentence = 0.644934\n",
      "\n",
      "real\t0m0.803s\n",
      "user\t0m0.620s\n",
      "sys\t0m0.028s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Segmented Corpus Post-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 796.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase segmented corpus written to ../datasets/vaccine/phrase_vaccine.txt\n"
     ]
    }
   ],
   "source": [
    "if args.override or (not os.path.exists(f\"datasets/{args.dataset}/phrase_{args.dataset}.txt\")):\n",
    "    if not os.path.exists(f\"datasets/{args.dataset}/{args.dataset}_text.txt\"):\n",
    "        pdf_files = []\n",
    "        for file in os.listdir(f\"datasets/{args.dataset}\"):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(f\"datasets/{args.dataset}\", file))\n",
    "        \n",
    "        with open(f\"datasets/{args.dataset}/{args.dataset}_text.txt\", \"w\") as f:\n",
    "            for file in pdf_files:\n",
    "                reader = PdfReader(file)\n",
    "                paper = []\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    page_text = re.sub(fr\"\\s+\", \" \", page_text)\n",
    "                    paper.append(page_text)\n",
    "                f.write(f\"{' '.join(paper)}\\n\")\n",
    "\n",
    "        shutil.copyfile(f\"datasets/{args.dataset}/{args.dataset}_text.txt\", f\"datasets/{args.dataset}/text.txt\")\n",
    "\n",
    "    # pre-process\n",
    "    os.chdir(\"./preprocessing\")\n",
    "    subprocess.check_call(['./auto_phrase.sh', args.dataset])\n",
    "    os.chdir(\"../\")\n",
    "else:\n",
    "    print(\"already pre-processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seed-guided topic discovery based on the aspects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = args.dataset.replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "if args.split:\n",
    "    chunk_len = 100 # words\n",
    "    window = 10 # words\n",
    "    with open(f\"datasets/{args.dataset}/phrase_{args.dataset}.txt\", \"r\") as fin, open(f\"SeeTopic/{dir_name}/{dir_name}.txt\", \"w\") as fout:\n",
    "        lines = [l.strip() for l in fin]\n",
    "        for line in lines:\n",
    "            all_words = line.split(\" \")\n",
    "            for chunk in range(0, len(all_words), chunk_len):\n",
    "                if chunk == 0:\n",
    "                    fout.write(f\"{' '.join(all_words[chunk:chunk+chunk_len])}\\n\")\n",
    "                else:\n",
    "                    fout.write(f\"{' '.join(all_words[chunk-window:chunk+chunk_len])}\\n\")\n",
    "\n",
    "else:\n",
    "    shutil.copyfile(f\"datasets/{args.dataset}/phrase_{args.dataset}.txt\", f\"SeeTopic/{dir_name}/{dir_name}.txt\")\n",
    "\n",
    "## aspects and seed terms\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(zip(aspects, keywords)):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(f\"SeeTopic/{args.dataset}\", f\"{dir_name}.txt\")) as fin:\n",
    "    lines = [l.strip() for l in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n",
      "####### CONSTRUCTING AND TOKENIZING VOCAB #######\n",
      "####### COMPUTING STATIC EMBEDDINGS #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.09it/s]\n",
      "100%|██████████| 10407/10407 [01:18<00:00, 133.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../vaccine/vaccine.txt\n",
      "Reading topics from file vaccine_1/keywords.txt\n",
      "Vocab size: 10371\n",
      "Words in train file: 394050\n",
      "Read 5 topics\n",
      "clinical_trials\teradicate\tvaccine_induced\t\n",
      "adverse_reactions\tside_effects\tallergic_reactions\t\n",
      "costco\tcost_effective\tinsured\t\n",
      "storage_requirements\ttransportation\thandling\t\n",
      "trust\tsocial_media\tattitude\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000285  Progress: 99.10%  Words/thread/sec: 14.09k  Topic mining results written to file vaccine_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../vaccine/vaccine.txt\n",
      "Reading topics from file vaccine_2/keywords.txt\n",
      "Vocab size: 10371\n",
      "Words in train file: 394050\n",
      "Read 5 topics\n",
      "clinical_trials\teradicate\tvaccine_induced\tcell_mediated\tinfectivity\tneutralize\t\n",
      "adverse_reactions\tside_effects\tallergic_reactions\tanaphylaxis\tserious_adverse_events\timmunogenicity\t\n",
      "costco\tinsured\tcost_effective\tincome\tbiased\tdistributors\t\n",
      "storage_requirements\ttransportation\thandling\tdistribution\tlogistics\troad\t\n",
      "trust\tsocial_media\tattitude\tpolicies\tawareness\tcampaign\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000284  Progress: 99.10%  Words/thread/sec: 13.64k  Topic mining results written to file vaccine_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../vaccine/vaccine.txt\n",
      "Reading topics from file vaccine_3/keywords.txt\n",
      "Vocab size: 10371\n",
      "Words in train file: 394050\n",
      "Read 5 topics\n",
      "clinical_trials\tcell_mediated\teradicate\tneutralize\tinfectivity\tvaccine_induced\tsera\tneutralization\tvaccinees\t\n",
      "adverse_reactions\tside_effects\tallergic_reactions\tanaphylaxis\tserious_adverse_events\timmunogenicity\ttolerability\treactogenicity\tpharmacovigilance\t\n",
      "costco\tbiased\tinsured\tcost_effective\tincome\tdistributors\tlower_income\tinequities\tinsurance\t\n",
      "storage_requirements\ttransportation\thandling\tdistribution\tlogistics\troad\tobstacles\tmaintenance\tmiddle_\t\n",
      "trust\tsocial_media\tattitude\tawareness\tpolicies\tcampaign\tstakeholders\toutreach\tviews\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000285  Progress: 99.10%  Words/thread/sec: 14.06k  Topic mining results written to file vaccine_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../vaccine/vaccine.txt\n",
      "Reading topics from file vaccine_4/keywords.txt\n",
      "Vocab size: 10371\n",
      "Words in train file: 394050\n",
      "Read 5 topics\n",
      "clinical_trials\tcell_mediated\tneutralize\teradicate\tinfectivity\tsera\tneutralization\tvaccine_induced\tneutralizing\tconvalescent\tb.1351\tvaccinees\t\n",
      "adverse_reactions\tside_effects\tallergic_reactions\tanaphylaxis\tserious_adverse_events\timmunogenicity\ttolerability\treactogenicity\tpharmacovigilance\tmunogenicity\tnausea/vomiting\tnonallergic\t\n",
      "costco\tbiased\tlower_income\tinequities\tdistributors\tinsured\tcost_effective\tincome\tinsurance\thigh_income\tdelays\tcosts\t\n",
      "storage_requirements\ttransportation\thandling\tdistribution\tlogistics\tobstacles\tmiddle_\troad\tmaintenance\tsampling\tusage\tlocation\t\n",
      "trust\tsocial_media\tattitude\tstakeholders\tawareness\toutreach\tpolicies\tpartners\tcampaign\tviews\thospitals\timplementation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000283  Progress: 99.10%  Words/thread/sec: 13.99k  Topic mining results written to file vaccine_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# os.chdir(\"../\")\n",
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', args.dataset, str(args.iters), args.model])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
